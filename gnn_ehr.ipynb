{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akshziitj/GNN_for_EHR/blob/master/gnn_ehr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-title"
      },
      "source": [
        "# GNN for EHR: Patient Clustering Demo\n",
        "\n",
        "This notebook demonstrates a complete workflow for clustering patients based on their EHR (Electronic Health Record) codes. It includes:\n",
        "1.  **Setup**: Installing libraries and checking the environment.\n",
        "2.  **Data Generation**: Creating a synthetic patient-code dataset.\n",
        "3.  **Data Exploration**: Calculating basic statistics.\n",
        "4.  **TF-IDF Baseline**: Running a standard machine learning baseline (TF-IDF + KMeans) and evaluating its Silhouette score.\n",
        "5.  **GNN Model**: Building, training, and generating embeddings using a Graph Attention Network (GATv2).\n",
        "6.  **GNN Evaluation**: Evaluating the GNN embeddings with KMeans and its Silhouette score.\n",
        "7.  **Analysis & Visualization**: Visualizing the results using t-SNE, heatmaps, and neighbor analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-setup-1"
      },
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-setup-2"
      },
      "source": [
        "### 1.1. Check GPU and Clone Repository\n",
        "\n",
        "We'll start by checking for a GPU, cloning the `GNN_for_EHR` repository, and navigating into its directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e37d868a"
      },
      "source": [
        "!nvidia-smi\n",
        "!rm -rf GNN_for_EHR\n",
        "!git clone https://github.com/NYUMedML/GNN_for_EHR.git\n",
        "%cd GNN_for_EHR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-setup-3"
      },
      "source": [
        "### 1.2. Install Dependencies\n",
        "\n",
        "Install the required libraries, primarily PyTorch and PyTorch Geometric (PyG)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ca516e"
      },
      "source": [
        "# Install PyTorch with CUDA 12.1 wheels\n",
        "!pip install --quiet --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Core PyG. Start minimal.\n",
        "!pip install --quiet torch_geometric\n",
        "\n",
        "# Optional: compiled extensions (faster, sometimes required)\n",
        "!pip install --quiet \\\n",
        "  pyg-lib torch-scatter torch-sparse torch-cluster torch-spline-conv \\\n",
        "  -f https://data.pyg.org/whl/torch-2.4.0+cu121.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-setup-4"
      },
      "source": [
        "### 1.3. Verify Installations\n",
        "\n",
        "Check that PyTorch and PyG are installed correctly and that CUDA is available."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "badef60f"
      },
      "source": [
        "import torch, torch_geometric\n",
        "print(\"Torch:\", torch.__version__, \"CUDA:\", torch.version.cuda, \"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"PyG:\", torch_geometric.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-data-1"
      },
      "source": [
        "## 2. Synthetic Data Generation\n",
        "\n",
        "This cell generates a toy corpus of 500 patients and 120 ICD-like codes. The data is structured with co-morbidity blocks (e.g., metabolic, cardiac, respiratory) to mimic real-world EHR data patterns. The generated data is saved to `data/sample/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "407813cf"
      },
      "source": [
        "import os, json, random, numpy as np, pathlib\n",
        "\n",
        "pathlib.Path(\"data/sample\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Toy corpus: 500 patients, 120 ICD-like codes. Co-morbidity blocks to create structure.\n",
        "rng = np.random.default_rng(7)\n",
        "num_pat=500; num_codes=120\n",
        "\n",
        "# Blocks: metabolic(0-19), cardiac(20-39), respiratory(40-59), neuro(60-79), misc(80-119)\n",
        "blocks = [(0,20),(20,40),(40,60),(60,80),(80,120)]\n",
        "patients = {}\n",
        "labels = {}  # binary outcome just for demo\n",
        "\n",
        "for pid in range(num_pat):\n",
        "    # pick a dominant block\n",
        "    b = rng.integers(0,4)\n",
        "    a,bhi = blocks[b]\n",
        "    # sample 3–7 codes from dominant + 0–3 from others\n",
        "    k_dom = rng.integers(3,8)\n",
        "    codes = rng.choice(np.arange(a,bhi), size=k_dom, replace=False).tolist()\n",
        "    for _ in range(rng.integers(0,4)):\n",
        "        ob = rng.integers(0,5)\n",
        "        oa,ohi = blocks[ob]\n",
        "        codes += rng.choice(np.arange(oa,ohi), size=rng.integers(1,3), replace=False).tolist()\n",
        "    codes = sorted(set(codes))\n",
        "    patients[str(pid)] = [int(c) for c in codes]\n",
        "    # synthetic outcome: higher risk if cardiac+metabolic present\n",
        "    labels[str(pid)] = int(any(20<=c<40 for c in codes) and any(0<=c<20 for c in codes))\n",
        "\n",
        "with open(\"data/sample/patient_to_codes.json\",\"w\") as f:\n",
        "    json.dump(patients, f)\n",
        "\n",
        "with open(\"data/sample/labels.json\",\"w\") as f:\n",
        "    json.dump(labels, f)\n",
        "\n",
        "print(\"Patients:\", len(patients), \"Example:\", list(patients.items())[:2])\n",
        "print(\"Positives:\", sum(labels.values()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-data-2"
      },
      "source": [
        "## 3. Data Exploration\n",
        "\n",
        "Load the generated data to calculate and print key statistics: total number of patients, number of unique codes, and the average number of codes per patient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "166b6577-combined"
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load the generated data\n",
        "with open(\"data/sample/patient_to_codes.json\", \"r\") as f:\n",
        "    p2c = json.load(f)\n",
        "\n",
        "# Calculate stats\n",
        "num_patients = len(p2c)\n",
        "all_codes = [code for codes in p2c.values() for code in codes]\n",
        "num_unique_codes = len(set(all_codes))\n",
        "total_codes = sum(len(codes) for codes in p2c.values())\n",
        "average_codes = total_codes / num_patients\n",
        "\n",
        "print(f\"Number of patients: {num_patients}\")\n",
        "print(f\"Number of unique ICD codes: {num_unique_codes}\")\n",
        "print(f\"Average codes per patient: {average_codes:.2f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-tfidf-1"
      },
      "source": [
        "## 4. Baseline Model: TF-IDF + KMeans\n",
        "\n",
        "This cell implements a standard machine learning baseline. It treats each patient's list of codes as a document, applies TF-IDF vectorization, and then performs KMeans clustering. Finally, it calculates the Silhouette score to evaluate the quality of the clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fba84b36"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load patient->codes\n",
        "with open(\"data/sample/patient_to_codes.json\") as f:\n",
        "    p2c = json.load(f)\n",
        "\n",
        "# Convert codes to string format for TF-IDF\n",
        "patient_docs = [\" \".join(map(str, codes)) for codes in p2c.values()]\n",
        "\n",
        "# Initialize and fit TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "X_tfidf = vectorizer.fit_transform(patient_docs)\n",
        "\n",
        "# Perform KMeans clustering (using 5 clusters as in the GNN example)\n",
        "kmeans_tfidf = KMeans(n_clusters=5, n_init=10, random_state=0)\n",
        "kmeans_tfidf.fit(X_tfidf)\n",
        "labels_tfidf = kmeans_tfidf.labels_\n",
        "\n",
        "# Calculate and print Silhouette score\n",
        "sil_tfidf = silhouette_score(X_tfidf, labels_tfidf)\n",
        "print(f\"TF-IDF KMeans Silhouette Score: {sil_tfidf:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-gnn-1"
      },
      "source": [
        "## 5. GNN Model: Training & Embedding Generation\n",
        "\n",
        "This cell builds and trains the Graph Neural Network (GNN) model.\n",
        "\n",
        "1.  **Graph Construction**: It builds a graph where nodes are disease codes. Edges are created between codes that co-occur in patient records, weighted by Positive Pointwise Mutual Information (PMI) to capture meaningful relationships.\n",
        "2.  **Model Definition**: A `GATv2Conv` (Graph Attention Network) model is defined.\n",
        "3.  **Unsupervised Training**: The model is trained using an unsupervised objective. It tries to pull the embeddings of connected (co-occurring) nodes closer together, weighted by their PMI score.\n",
        "4.  **Embedding Generation**:\n",
        "    * `disease_embeddings.npy`: The trained GNN generates embeddings for each disease code.\n",
        "    * `patient_embeddings.npy`: Patient embeddings are created by taking a weighted average of the embeddings of the diseases they have. The weight is based on a simple heuristic (L2 norm of the disease embedding, passed through a softmax) to give more importance to more \"significant\" disease embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8240f3ac"
      },
      "source": [
        "import json, numpy as np, torch\n",
        "from torch import nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GATv2Conv\n",
        "from itertools import combinations\n",
        "import os\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load patient->codes (already in memory as p2c)\n",
        "with open(\"data/sample/patient_to_codes.json\") as f:\n",
        "    p2c = json.load(f)\n",
        "\n",
        "num_codes = max(c for codes in p2c.values() for c in codes) + 1\n",
        "\n",
        "# Build co-occurrence edges and PMI weights\n",
        "code_freq = Counter()\n",
        "pair_freq = Counter()\n",
        "num_pat = len(p2c)\n",
        "\n",
        "for codes in p2c.values():\n",
        "    codes = sorted(set(codes))\n",
        "    for c in codes:\n",
        "        code_freq[c]+=1\n",
        "    for i,j in combinations(codes,2):\n",
        "        pair = (i,j) if i<j else (j,i)\n",
        "        pair_freq[pair]+=1\n",
        "\n",
        "edges_i, edges_j, weights = [], [], []\n",
        "for (i,j), n_ij in pair_freq.items():\n",
        "    p_ij = n_ij/num_pat\n",
        "    p_i = code_freq[i]/num_pat\n",
        "    p_j = code_freq[j]/num_pat\n",
        "    pmi = np.log((p_ij + 1e-9)/(p_i*p_j + 1e-12))\n",
        "    if pmi>0:\n",
        "        edges_i += [i,j]\n",
        "        edges_j += [j,i]\n",
        "        w = float(pmi)\n",
        "        weights += [w,w]\n",
        "\n",
        "edge_index = torch.tensor([edges_i, edges_j], dtype=torch.long)\n",
        "edge_weight = torch.tensor(weights, dtype=torch.float)\n",
        "\n",
        "x = nn.Embedding(num_codes, 64).weight.detach()\n",
        "data = Data(x=x, edge_index=edge_index, edge_weight=edge_weight).to(device)\n",
        "\n",
        "class GATModel(nn.Module):\n",
        "    def __init__(self, in_dim=64, hid=128, out_dim=64, heads=4, dr=0.3):\n",
        "        super().__init__()\n",
        "        self.lin = nn.Linear(in_dim, in_dim)\n",
        "        self.g1 = GATv2Conv(in_dim, hid//heads, heads=heads, dropout=dr)\n",
        "        self.g2 = GATv2Conv(hid, out_dim//heads, heads=heads, dropout=dr, concat=True)\n",
        "        self.act = nn.ELU()\n",
        "        self.dr = nn.Dropout(dr)\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.lin(x)\n",
        "        x = self.dr(self.act(self.g1(x, edge_index)))\n",
        "        x = self.g2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "model = GATModel().to(device)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-5)\n",
        "\n",
        "print(f\"Training GNN on {device}...\")\n",
        "model.train()\n",
        "for epoch in range(50):\n",
        "    opt.zero_grad()\n",
        "    z = model(data.x, data.edge_index)\n",
        "    # Unsupervised objective: pull connected nodes together (weighted)\n",
        "    i,j = data.edge_index\n",
        "    loss = ((z[i]-z[j]).pow(2).sum(1) * edge_weight.to(device)).mean()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    if epoch%10==0:\n",
        "        print(f\"epoch {epoch} loss {loss.item():.4f}\")\n",
        "\n",
        "emb = model(data.x, data.edge_index).detach().cpu().numpy()\n",
        "np.save(\"disease_embeddings.npy\", emb)\n",
        "\n",
        "# Patient embeddings: attention-weighted mean with simple learned scorer\n",
        "w = nn.Parameter(torch.zeros(emb.shape[1]))\n",
        "with torch.no_grad():\n",
        "    # score = w^T h; start with uniform since w=0\n",
        "    pass\n",
        "\n",
        "pvecs = {}\n",
        "for pid, codes in p2c.items():\n",
        "    if not codes:\n",
        "        continue\n",
        "    H = torch.tensor(emb[codes], dtype=torch.float)\n",
        "    # softmax over L2 norm as heuristic importance\n",
        "    scores = torch.linalg.norm(H, dim=1)\n",
        "    alpha = F.softmax(scores, dim=0).unsqueeze(1)\n",
        "    hp = (alpha*H).sum(0).numpy()\n",
        "    pvecs[pid] = hp\n",
        "\n",
        "np.save(\"patient_embeddings.npy\", pvecs)\n",
        "print(\"Saved: disease_embeddings.npy, patient_embeddings.npy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-gnn-2"
      },
      "source": [
        "## 6. GNN Model: Evaluation (Clustering)\n",
        "\n",
        "This cell loads the `patient_embeddings.npy` file generated by the GNN, performs KMeans clustering (with 5 clusters, same as the baseline), and reports the Silhouette score. This allows for a direct comparison with the TF-IDF baseline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP47WSrrEakW-cluster"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Load patient embeddings\n",
        "pmap = np.load(\"patient_embeddings.npy\", allow_pickle=True).item()\n",
        "X_gnn = np.vstack(list(pmap.values()))\n",
        "\n",
        "# Run KMeans\n",
        "kmeans_gnn = KMeans(n_clusters=5, n_init=10, random_state=0).fit(X_gnn)\n",
        "labels_gnn = kmeans_gnn.labels_\n",
        "\n",
        "# Calculate and print Silhouette score\n",
        "sil_gnn = silhouette_score(X_gnn, labels_gnn)\n",
        "print(f\"GNN KMeans Silhouette Score: {sil_gnn:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-1"
      },
      "source": [
        "## 7. Analysis & Visualization\n",
        "\n",
        "Now we use the embeddings and cluster labels to understand the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-2"
      },
      "source": [
        "### 7.1. t-SNE of Disease Embeddings\n",
        "\n",
        "This plot visualizes the `disease_embeddings.npy` in 2D space using t-SNE. We hope to see that codes from the same synthetic block (e.g., all cardiac codes) cluster together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joykkOjiF78b"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "E = np.load(\"disease_embeddings.npy\")              # shape: [num_codes, d]\n",
        "tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=0)\n",
        "Z = tsne.fit_transform(E)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(Z[:,0], Z[:,1], s=12)\n",
        "plt.title(\"t-SNE of Disease Embeddings\")\n",
        "plt.xlabel(\"t-SNE 1\"); plt.ylabel(\"t-SNE 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_disease_tsne.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-3"
      },
      "source": [
        "### 7.2. t-SNE of Patient Embeddings\n",
        "\n",
        "This plot visualizes the `patient_embeddings.npy` in 2D space, with each point colored by the cluster ID assigned by KMeans (using the `labels_gnn` from the previous step)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JN6_ZhpiGAxc"
      },
      "source": [
        "# Note: This cell re-runs KMeans, but we could also use the 'labels_gnn' from cell 6.\n",
        "# For self-contained visualization, we'll follow the original cell's logic.\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pe = np.load(\"patient_embeddings.npy\", allow_pickle=True).item()\n",
        "P_ids = list(pe.keys())\n",
        "X = np.vstack([pe[k] for k in P_ids])\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, n_init=10, random_state=0).fit(X)\n",
        "labels = kmeans.labels_\n",
        "\n",
        "tsne = TSNE(n_components=2, init=\"pca\", learning_rate=\"auto\", perplexity=30, random_state=0)\n",
        "Zp = tsne.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.scatter(Zp[:,0], Zp[:,1], s=12, c=labels)\n",
        "plt.title(\"t-SNE of Patient Embeddings (KMeans labels)\")\n",
        "plt.xlabel(\"t-SNE 1\"); plt.ylabel(\"t-SNE 2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_patient_tsne_kmeans.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-4"
      },
      "source": [
        "### 7.3. Disease Co-occurrence Subgraph\n",
        "\n",
        "This visualization constructs a graph based on the raw co-occurrence frequencies (not PMI) and displays the top 50 most frequent co-occurring disease pairs. This helps visualize the raw structure of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGVk_yREGE1r"
      },
      "source": [
        "import json, numpy as np\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "\n",
        "with open(\"data/sample/patient_to_codes.json\") as f:\n",
        "    p2c = json.load(f)\n",
        "\n",
        "num_pat = len(p2c)\n",
        "code_freq = Counter()\n",
        "pair_freq = Counter()\n",
        "for codes in p2c.values():\n",
        "    codes = sorted(set(codes))\n",
        "    for c in codes: code_freq[c]+=1\n",
        "    for i,j in combinations(codes,2):\n",
        "        if i>j: i,j=j,i\n",
        "        pair_freq[(i,j)]+=1\n",
        "\n",
        "# keep top 50 edges by frequency\n",
        "top_pairs = sorted(pair_freq.items(), key=lambda x: x[1], reverse=True)[:50]\n",
        "G = nx.Graph()\n",
        "for (i,j), w in top_pairs:\n",
        "    G.add_edge(i,j,weight=w)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "pos = nx.spring_layout(G, seed=0)\n",
        "nx.draw_networkx_nodes(G, pos, node_size=80)\n",
        "nx.draw_networkx_edges(G, pos, width=[0.5+0.2*G[u][v]['weight'] for u,v in G.edges()])\n",
        "# label only high-degree nodes\n",
        "deg = dict(G.degree())\n",
        "lbls = {n:str(n) for n,d in deg.items() if d>=3}\n",
        "nx.draw_networkx_labels(G, pos, labels=lbls, font_size=7)\n",
        "plt.title(\"Top Disease Co-occurrence Subgraph\")\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_disease_cooccurrence_graph.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-5"
      },
      "source": [
        "### 7.4. Disease & Patient Neighbor Analysis\n",
        "\n",
        "This cell performs two types of analysis:\n",
        "1.  **Disease Neighbors**: Finds the top 5 most similar diseases for a few sample diseases (5, 25, 65) based on the cosine similarity of their GNN embeddings.\n",
        "2.  **Patient Neighbors**: Finds the 5 nearest neighbors for the first 5 patients based on the cosine similarity of their GNN patient embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP47WSrrEakW-neighbors"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "emb = np.load(\"disease_embeddings.npy\")\n",
        "pmap = np.load(\"patient_embeddings.npy\", allow_pickle=True).item()\n",
        "X = np.vstack(list(pmap.values()))\n",
        "\n",
        "print(\"--- Disease Neighbor Analysis (Cosine Similarity) ---\")\n",
        "sim = cosine_similarity(emb)\n",
        "for idx in [5, 25, 65]:\n",
        "    # find top 5 neighbors (excluding self)\n",
        "    nn = sim[idx].argsort()[-6:-1][::-1]\n",
        "    print(f\"Disease {idx} neighbors:\", nn)\n",
        "\n",
        "print(\"\n",
        "--- Patient Neighbor Analysis (k-NN) ---\")\n",
        "nbrs = NearestNeighbors(n_neighbors=5, metric=\"cosine\").fit(X)\n",
        "dist, ind = nbrs.kneighbors(X[:5])\n",
        "print(\"Patient neighbor indices for first 5 rows:\\n\", ind)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-6"
      },
      "source": [
        "### 7.5. Patient-Patient Similarity Heatmap\n",
        "\n",
        "This visualization shows a heatmap of the cosine similarity matrix for the first 50 patients, giving a dense view of patient similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJyN-Zv4GI7i"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "pe = np.load(\"patient_embeddings.npy\", allow_pickle=True).item()\n",
        "X = np.vstack(list(pe.values()))\n",
        "X50 = X[:50]\n",
        "S = cosine_similarity(X50)\n",
        "\n",
        "plt.figure(figsize=(6,5))\n",
        "plt.imshow(S, aspect=\"auto\")\n",
        "plt.title(\"Patient–Patient Cosine Similarity (first 50)\")\n",
        "plt.xlabel(\"Patient Index\"); plt.ylabel(\"Patient Index\")\n",
        "plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_patient_similarity_heatmap.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-viz-7"
      },
      "source": [
        "### 7.6. Heuristic Disease Contributions\n",
        "\n",
        "This plot shows the top 10 diseases contributing to Patient 0's final embedding, based on the heuristic \"attention\" score (softmax of the L2 norm) used during the patient embedding generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC10azLWGO5B"
      },
      "source": [
        "import json, numpy as np, matplotlib.pyplot as plt\n",
        "pid = \"0\"  # change as needed\n",
        "\n",
        "E = np.load(\"disease_embeddings.npy\")\n",
        "with open(\"data/sample/patient_to_codes.json\") as f:\n",
        "    p2c = json.load(f)\n",
        "codes = p2c[pid]\n",
        "\n",
        "# heuristic \"attention\": L2 norm of disease embeddings followed by softmax\n",
        "H = E[codes]\n",
        "scores = np.linalg.norm(H, axis=1)\n",
        "alpha = np.exp(scores) / np.sum(np.exp(scores))\n",
        "\n",
        "order = np.argsort(alpha)[::-1][:10]\n",
        "top_codes = [str(codes[i]) for i in order]\n",
        "top_alpha = alpha[order]\n",
        "\n",
        "plt.figure(figsize=(7,3))\n",
        "plt.bar(range(len(top_codes)), top_alpha)\n",
        "plt.xticks(range(len(top_codes)), top_codes, rotation=0)\n",
        "plt.title(f\"Top-10 Disease Contributions for Patient {pid}\")\n",
        "plt.xlabel(\"Disease Code\"); plt.ylabel(\"Weight\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"fig_patient_attention_bars.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md-drive-1"
      },
      "source": [
        "## 8. (Optional) Save Artifacts to Google Drive\n",
        "\n",
        "This cell mounts Google Drive and copies all generated files (embeddings, figures, data) into a new directory for persistence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZOIgbTWEgm2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!mkdir -p '/content/drive/MyDrive/gnn_ehr_demo'\n",
        "!cp -r *.npy data/sample *.png '/content/drive/MyDrive/gnn_ehr_demo/' 2>/dev/null || true\n",
        "print(\"Copied artifacts to /content/drive/MyDrive/gnn_ehr_demo/\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}